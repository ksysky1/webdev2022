# 1007 학습내용
## 인공지능 활성화 함수
- Sigmoid : 2진으로 나눌 때 사용. 출력층에서 사용. 입력값이 무한대로 커져도 모델 계층이 많을수록 gradient값이 0에 수렴해버림.(Vanishing gradient 문제)

- Relu : 2진으로 나눌 때, 다중 분류할 때 둘다 사용. 중간층에서 사용. 0이상이면 그 수 그대로 출력. 0 이하이면 0을 출력.

- Softmax : 다중 분류시 사용. 결과값을 확률로 바꿔줌. 분류해야할 클래스가 3개면 3개의 총합을 1로 만들어줌. 3개 중 확률이 가장 높게 나온 걸로 예측함.
